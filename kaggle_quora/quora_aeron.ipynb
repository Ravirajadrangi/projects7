{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "TODO. add deepnet and new features from:  \n",
    "https://github.com/abhishekkrthakur/is_that_a_duplicate_quora_question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import log_loss\n",
    "from scipy.optimize import minimize\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "import xgboost as xgb\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials \n",
    "import os\n",
    "\n",
    "import xgboost as xgb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> averaged_perceptron_tagger\n",
      "    Downloading package averaged_perceptron_tagger to\n",
      "        /home/ubuntu/nltk_data...\n",
      "      Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "cleaning, feature engineering, commonize feature train-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_csv():\n",
    "    global train, test\n",
    "    train = pd.read_csv('./data/train.csv') #remove limit\n",
    "    test = pd.read_csv('./data/test.csv') #remove limit\n",
    "    \n",
    "def word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_words_in_q1 = [w for w in q1words.keys() if w in q2words]\n",
    "    shared_words_in_q2 = [w for w in q2words.keys() if w in q1words]\n",
    "    R = (len(shared_words_in_q1) + len(shared_words_in_q2))/(len(q1words) + len(q2words))\n",
    "    return R\n",
    "\n",
    "def get_weight(count, eps=500, min_count=2):\n",
    "    if count < min_count:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1 / (count + eps)\n",
    "\n",
    "def common_words():\n",
    "    global weights\n",
    "    train_qs = pd.Series(train['question1'].tolist() + train['question2'].tolist()).astype(str)\n",
    "    test_qs = pd.Series(test['question1'].tolist() + test['question2'].tolist()).astype(str)\n",
    "    words = (\" \".join(train_qs)).lower().split() + (\" \".join(test_qs)).lower().split()\n",
    "    counts = Counter(words)\n",
    "    weights = {word: get_weight(count) for word, count in counts.items()}\n",
    "\n",
    "def tfidf_word_match_share(row):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in str(row['question1']).lower().split():\n",
    "        if word not in stops:\n",
    "            q1words[word] = 1\n",
    "    for word in str(row['question2']).lower().split():\n",
    "        if word not in stops:\n",
    "            q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        return 0\n",
    "    shared_weights = [weights.get(w, 0) for w in q1words.keys() if w in q2words] + [weights.get(w, 0) for w in q2words.keys() if w in q1words]\n",
    "    total_weights = [weights.get(w, 0) for w in q1words] + [weights.get(w, 0) for w in q2words]\n",
    "    \n",
    "    R = np.sum(shared_weights) / np.sum(total_weights)\n",
    "    return R\n",
    "\n",
    "def get_unigrams(que):\n",
    "    return [word for word in nltk.word_tokenize(que.lower()) if word not in stops]\n",
    "\n",
    "def get_common_unigrams(row):\n",
    "    return len( set(row[\"unigrams_ques1\"]).intersection(set(row[\"unigrams_ques2\"])) )\n",
    "\n",
    "def get_common_unigram_ratio(row):\n",
    "    return float(row[\"zunigrams_common_count\"]) / max(len( set(row[\"unigrams_ques1\"]).union(set(row[\"unigrams_ques2\"])) ),1)\n",
    "\n",
    "def get_bigrams(que):\n",
    "    return [i for i in nltk.ngrams(que, 2)]\n",
    "\n",
    "def get_common_bigrams(row):\n",
    "    return len( set(row[\"bigrams_ques1\"]).intersection(set(row[\"bigrams_ques2\"])) )\n",
    "\n",
    "def get_common_bigram_ratio(row):\n",
    "    return float(row[\"zbigrams_common_count\"]) / max(len( set(row[\"bigrams_ques1\"]).union(set(row[\"bigrams_ques2\"])) ),1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feats_train():\n",
    "    train['test'] = train.question1.map(lambda x: [t for  t in nltk.pos_tag(nltk.word_tokenize(str(x).lower()))])\n",
    "    train['test1'] = train.question2.map(lambda x: [t for  t in nltk.pos_tag(nltk.word_tokenize(str(x).lower()))])\n",
    "\n",
    "    train['question1_nouns'] = train.test.map(lambda x: [w for w, t in x if t == 'NN'])\n",
    "    train['question2_nouns'] = train.test1.map(lambda x: [w for w, t in x if t == 'NN'])\n",
    "\n",
    "    train['question1_proper_nouns'] = train.test.map(lambda x: [w for w, t in x if t[:3] in ['NNP']])\n",
    "    train['question2_proper_nouns'] = train.test1.map(lambda x: [w for w, t in x if t[:3] in ['NNP']])\n",
    "\n",
    "    train['question1_verbs'] = train.test.map(lambda x: [w for w, t in x if t[:2] in ['VB']])\n",
    "    train['question2_verbs'] = train.test1.map(lambda x: [w for w, t in x if t[:2] in ['VB']])\n",
    "\n",
    "    train['question1_jj'] = train.test.map(lambda x: [w for w, t in x if t[:2] in ['JJ']])\n",
    "    train['question2_jj'] = train.test1.map(lambda x: [w for w, t in x if t[:2] in ['JJ']])\n",
    "\n",
    "    train['question1_rb'] = train.test.map(lambda x: [w for w, t in x if t[:2] in ['RB']])\n",
    "    train['question2_rb'] = train.test1.map(lambda x: [w for w, t in x if t[:2] in ['RB']])\n",
    "\n",
    "    train['question1_cc'] = train.test.map(lambda x: [w for w, t in x if t == 'CC'])\n",
    "    train['question2_cc'] = train.test1.map(lambda x: [w for w, t in x if t == 'CC'])\n",
    "\n",
    "    train['question1_cd'] = train.test.map(lambda x: [w for w, t in x if t == 'CD'])\n",
    "    train['question2_cd'] = train.test1.map(lambda x: [w for w, t in x if t == 'CD'])\n",
    "\n",
    "    train['question1_dt'] = train.test.map(lambda x: [w for w, t in x if t == 'DT'])\n",
    "    train['question2_dt'] = train.test1.map(lambda x: [w for w, t in x if t == 'DT'])\n",
    "\n",
    "    train['question1_ex'] = train.test.map(lambda x: [w for w, t in x if t == 'EX'])\n",
    "    train['question2_ex'] = train.test1.map(lambda x: [w for w, t in x if t == 'EX'])\n",
    "\n",
    "    train['question1_ls'] = train.test.map(lambda x: [w for w, t in x if t == 'LS'])\n",
    "    train['question2_ls'] = train.test1.map(lambda x: [w for w, t in x if t == 'LS'])\n",
    "\n",
    "    train['question1_wdt'] = train.test.map(lambda x: [w for w, t in x if t == 'WDT'])\n",
    "    train['question2_wdt'] = train.test1.map(lambda x: [w for w, t in x if t == 'WDT'])\n",
    "\n",
    "    train['z_word_len1'] = train.question1.map(lambda x: len(str(x).split()))\n",
    "    train['z_word_len2'] = train.question2.map(lambda x: len(str(x).split()))\n",
    "    train['z_noun_match'] = train.apply(lambda r: sum([1 for w in r.question1_nouns if w in r.question2_nouns]), axis=1)\n",
    "    train['z_verb_match'] = train.apply(lambda r: sum([1 for w in r.question1_verbs if w in r.question2_verbs]), axis=1)\n",
    "    train['z_prop_noun_match'] = train.apply(lambda r: sum([1 for w in r.question1_proper_nouns if w in r.question2_proper_nouns]), axis=1)\n",
    "    train['z_cc_match'] = train.apply(lambda r: sum([1 for w in r.question1_cc if w in r.question2_cc]), axis=1)\n",
    "    train['z_cd_match'] = train.apply(lambda r: sum([1 for w in r.question1_cd if w in r.question2_cd]), axis=1)\n",
    "    train['z_dt_match'] = train.apply(lambda r: sum([1 for w in r.question1_dt if w in r.question2_dt]), axis=1)\n",
    "    train['z_jj_match'] = train.apply(lambda r: sum([1 for w in r.question1_jj if w in r.question2_jj]), axis=1)\n",
    "    train['z_rb_match'] = train.apply(lambda r: sum([1 for w in r.question1_rb if w in r.question2_rb]), axis=1)\n",
    "    train['z_ex_match'] = train.apply(lambda r: sum([1 for w in r.question1_ex if w in r.question2_ex]), axis=1)\n",
    "    train['z_ls_match'] = train.apply(lambda r: sum([1 for w in r.question1_ls if w in r.question2_ls]), axis=1)\n",
    "    train['z_wdt_match'] = train.apply(lambda r: sum([1 for w in r.question1_wdt if w in r.question2_wdt]), axis=1)\n",
    "\n",
    "\n",
    "    #train['nr_nouns'] = train.question1.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
    "    train['total_nouns'] = train['question1_nouns'] + train['question2_nouns']\n",
    "    train['total_verbs'] = train['question1_verbs'] + train['question2_verbs']\n",
    "    train['total_proper_nouns'] = train['question1_proper_nouns'] + train['question2_proper_nouns']\n",
    "    train['total_cc'] = train['question1_cc'] + train['question2_cc']\n",
    "    train['total_cd'] = train['question1_cd'] + train['question2_cd']\n",
    "    train['total_dt'] = train['question1_dt'] + train['question2_dt']\n",
    "    train['total_jj'] = train['question1_jj'] + train['question2_jj']\n",
    "    train['total_rb'] = train['question1_rb'] + train['question2_rb']\n",
    "    train['total_ex'] = train['question1_ex'] + train['question2_ex']\n",
    "    train['total_ls'] = train['question1_ls'] + train['question2_ls']\n",
    "    train['total_wdt'] = train['question1_wdt'] + train['question2_wdt']\n",
    "\n",
    "    train['z_nr_nouns'] = train['total_nouns'].map(lambda x: len(x))\n",
    "    train['z_nr_verbs'] = train['total_verbs'].map(lambda x: len(x))\n",
    "    train['z_nr_proper_nouns'] = train['total_proper_nouns'].map(lambda x: len(x))\n",
    "    train['z_nr_cc'] = train['total_cc'].map(lambda x: len(x))\n",
    "    train['z_nr_cd'] = train['total_cd'].map(lambda x: len(x))\n",
    "    train['z_nr_dt'] = train['total_dt'].map(lambda x: len(x))\n",
    "    train['z_nr_jj'] = train['total_jj'].map(lambda x: len(x))\n",
    "    train['z_nr_rb'] = train['total_rb'].map(lambda x: len(x))\n",
    "    train['z_nr_ex'] = train['total_ex'].map(lambda x: len(x))\n",
    "    train['z_nr_ls'] = train['total_ls'].map(lambda x: len(x))\n",
    "    train['z_nr_wdt'] = train['total_wdt'].map(lambda x: len(x))\n",
    "\n",
    "\n",
    "    train['z_word_match'] = train.apply(word_match_share, axis=1, raw=True)\n",
    "    train['z_tfidf_word_match'] = train.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "\n",
    "    train[\"unigrams_ques1\"] = train['question1'].apply(lambda x: get_unigrams(str(x)))\n",
    "    train[\"unigrams_ques2\"] = train['question2'].apply(lambda x: get_unigrams(str(x)))\n",
    "    train[\"zunigrams_common_count\"] = train.apply(lambda r: get_common_unigrams(r),axis=1)\n",
    "    train[\"zunigrams_common_ratio\"] = train.apply(lambda r: get_common_unigram_ratio(r), axis=1)\n",
    "    train[\"bigrams_ques1\"] = train[\"unigrams_ques1\"].apply(lambda x: get_bigrams(x))\n",
    "    train[\"bigrams_ques2\"] = train[\"unigrams_ques2\"].apply(lambda x: get_bigrams(x)) \n",
    "    train[\"zbigrams_common_count\"] = train.apply(lambda r: get_common_bigrams(r),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def feats_test():\n",
    "    test['test'] = test.question1.map(lambda x: [t for  t in nltk.pos_tag(nltk.word_tokenize(str(x).lower()))])\n",
    "    test['test1'] = test.question2.map(lambda x: [t for  t in nltk.pos_tag(nltk.word_tokenize(str(x).lower()))])\n",
    "\n",
    "    test['question1_nouns'] = test.test.map(lambda x: [w for w, t in x if t == 'NN'])\n",
    "    test['question2_nouns'] = test.test1.map(lambda x: [w for w, t in x if t == 'NN'])\n",
    "\n",
    "    test['question1_proper_nouns'] = test.test.map(lambda x: [w for w, t in x if t[:3] in ['NNP']])\n",
    "    test['question2_proper_nouns'] = test.test1.map(lambda x: [w for w, t in x if t[:3] in ['NNP']])\n",
    "\n",
    "    test['question1_verbs'] = test.test.map(lambda x: [w for w, t in x if t[:2] in ['VB']])\n",
    "    test['question2_verbs'] = test.test1.map(lambda x: [w for w, t in x if t[:2] in ['VB']])\n",
    "\n",
    "    test['question1_jj'] = test.test.map(lambda x: [w for w, t in x if t[:2] in ['JJ']])\n",
    "    test['question2_jj'] = test.test1.map(lambda x: [w for w, t in x if t[:2] in ['JJ']])\n",
    "\n",
    "    test['question1_rb'] = test.test.map(lambda x: [w for w, t in x if t[:2] in ['RB']])\n",
    "    test['question2_rb'] = test.test1.map(lambda x: [w for w, t in x if t[:2] in ['RB']])\n",
    "\n",
    "    test['question1_cc'] = test.test.map(lambda x: [w for w, t in x if t == 'CC'])\n",
    "    test['question2_cc'] = test.test1.map(lambda x: [w for w, t in x if t == 'CC'])\n",
    "\n",
    "    test['question1_cd'] = test.test.map(lambda x: [w for w, t in x if t == 'CD'])\n",
    "    test['question2_cd'] = test.test1.map(lambda x: [w for w, t in x if t == 'CD'])\n",
    "\n",
    "    test['question1_dt'] = test.test.map(lambda x: [w for w, t in x if t == 'DT'])\n",
    "    test['question2_dt'] = test.test1.map(lambda x: [w for w, t in x if t == 'DT'])\n",
    "\n",
    "    test['question1_ex'] = test.test.map(lambda x: [w for w, t in x if t == 'EX'])\n",
    "    test['question2_ex'] = test.test1.map(lambda x: [w for w, t in x if t == 'EX'])\n",
    "\n",
    "    test['question1_ls'] = test.test.map(lambda x: [w for w, t in x if t == 'LS'])\n",
    "    test['question2_ls'] = test.test1.map(lambda x: [w for w, t in x if t == 'LS'])\n",
    "\n",
    "    test['question1_wdt'] = test.test.map(lambda x: [w for w, t in x if t == 'WDT'])\n",
    "    test['question2_wdt'] = test.test1.map(lambda x: [w for w, t in x if t == 'WDT'])\n",
    "\n",
    "    test['z_word_len1'] = test.question1.map(lambda x: len(str(x).split()))\n",
    "    test['z_word_len2'] = test.question2.map(lambda x: len(str(x).split()))\n",
    "    test['z_noun_match'] = test.apply(lambda r: sum([1 for w in r.question1_nouns if w in r.question2_nouns]), axis=1)\n",
    "    test['z_verb_match'] = test.apply(lambda r: sum([1 for w in r.question1_verbs if w in r.question2_verbs]), axis=1)\n",
    "    test['z_prop_noun_match'] = test.apply(lambda r: sum([1 for w in r.question1_proper_nouns if w in r.question2_proper_nouns]), axis=1)\n",
    "    test['z_cc_match'] = test.apply(lambda r: sum([1 for w in r.question1_cc if w in r.question2_cc]), axis=1)\n",
    "    test['z_cd_match'] = test.apply(lambda r: sum([1 for w in r.question1_cd if w in r.question2_cd]), axis=1)\n",
    "    test['z_dt_match'] = test.apply(lambda r: sum([1 for w in r.question1_dt if w in r.question2_dt]), axis=1)\n",
    "    test['z_jj_match'] = test.apply(lambda r: sum([1 for w in r.question1_jj if w in r.question2_jj]), axis=1)\n",
    "    test['z_rb_match'] = test.apply(lambda r: sum([1 for w in r.question1_rb if w in r.question2_rb]), axis=1)\n",
    "    test['z_ex_match'] = test.apply(lambda r: sum([1 for w in r.question1_ex if w in r.question2_ex]), axis=1)\n",
    "    test['z_ls_match'] = test.apply(lambda r: sum([1 for w in r.question1_ls if w in r.question2_ls]), axis=1)\n",
    "    test['z_wdt_match'] = test.apply(lambda r: sum([1 for w in r.question1_wdt if w in r.question2_wdt]), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    #train['nr_nouns'] = train.question1.map(lambda x: [w for w, t in nltk.pos_tag(nltk.word_tokenize(str(x).lower())) if t[:1] in ['N']])\n",
    "    test['total_nouns'] = test['question1_nouns'] + test['question2_nouns']\n",
    "    test['total_verbs'] = test['question1_verbs'] + test['question2_verbs']\n",
    "    test['total_proper_nouns'] = test['question1_proper_nouns'] + test['question2_proper_nouns']\n",
    "    test['total_cc'] = test['question1_cc'] + test['question2_cc']\n",
    "    test['total_cd'] = test['question1_cd'] + test['question2_cd']\n",
    "    test['total_dt'] = test['question1_dt'] + test['question2_dt']\n",
    "    test['total_jj'] = test['question1_jj'] + test['question2_jj']\n",
    "    test['total_rb'] = test['question1_rb'] + test['question2_rb']\n",
    "    test['total_ex'] = test['question1_ex'] + test['question2_ex']\n",
    "    test['total_ls'] = test['question1_ls'] + test['question2_ls']\n",
    "    test['total_wdt'] = test['question1_wdt'] + test['question2_wdt']\n",
    "\n",
    "    test['z_nr_nouns'] = test['total_nouns'].map(lambda x: len(x))\n",
    "    test['z_nr_verbs'] = test['total_verbs'].map(lambda x: len(x))\n",
    "    test['z_nr_proper_nouns'] = test['total_proper_nouns'].map(lambda x: len(x))\n",
    "    test['z_nr_cc'] = test['total_cc'].map(lambda x: len(x))\n",
    "    test['z_nr_cd'] = test['total_cd'].map(lambda x: len(x))\n",
    "    test['z_nr_dt'] = test['total_dt'].map(lambda x: len(x))\n",
    "    test['z_nr_jj'] = test['total_jj'].map(lambda x: len(x))\n",
    "    test['z_nr_rb'] = test['total_rb'].map(lambda x: len(x))\n",
    "    test['z_nr_ex'] = test['total_ex'].map(lambda x: len(x))\n",
    "    test['z_nr_ls'] = test['total_ls'].map(lambda x: len(x))\n",
    "    test['z_nr_wdt'] = test['total_wdt'].map(lambda x: len(x))\n",
    "\n",
    "\n",
    "    test['z_word_match'] = test.apply(word_match_share, axis=1, raw=True)\n",
    "    test['z_tfidf_word_match'] = test.apply(tfidf_word_match_share, axis=1, raw=True)\n",
    "\n",
    "    test[\"unigrams_ques1\"] = test['question1'].apply(lambda x: get_unigrams(str(x)))\n",
    "    test[\"unigrams_ques2\"] = test['question2'].apply(lambda x: get_unigrams(str(x)))\n",
    "    test[\"zunigrams_common_count\"] = test.apply(lambda r: get_common_unigrams(r),axis=1)\n",
    "    test[\"zunigrams_common_ratio\"] = test.apply(lambda r: get_common_unigram_ratio(r), axis=1)\n",
    "    test[\"bigrams_ques1\"] = test[\"unigrams_ques1\"].apply(lambda x: get_bigrams(x))\n",
    "    test[\"bigrams_ques2\"] = test[\"unigrams_ques2\"].apply(lambda x: get_bigrams(x)) \n",
    "    test[\"zbigrams_common_count\"] = test.apply(lambda r: get_common_bigrams(r),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def scale_data():\n",
    "    global train, test, col\n",
    "    train = train.fillna(-1)\n",
    "    test = test.fillna(-1)\n",
    "\n",
    "    col = [c for c in train.columns if c[:1]=='z']\n",
    "\n",
    "    pos_train = train[train['is_duplicate'] == 1]\n",
    "    neg_train = train[train['is_duplicate'] == 0]\n",
    "    p = 0.165\n",
    "    scale = ((len(pos_train) / (len(pos_train) + len(neg_train))) / p) - 1\n",
    "    while scale > 1:\n",
    "        neg_train = pd.concat([neg_train, neg_train])\n",
    "        scale -=1\n",
    "    neg_train = pd.concat([neg_train, neg_train[:int(scale * len(neg_train))]])\n",
    "    train = pd.concat([pos_train, neg_train])\n",
    "    \n",
    "def save_processed_pickle():\n",
    "    pd.to_pickle(train, \"./data/train.pkl\")\n",
    "    pd.to_pickle(test, \"./data/train.pkl\")\n",
    "    \n",
    "def load_processed_pickle():\n",
    "    global train, test\n",
    "    print(\"Loading train and test pickles. Preprocessed versions.\")\n",
    "    train = pd.read_pickle(\"./data/train.pkl\")\n",
    "    test = pd.read_pickle(\"./data/test.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel/__main__.py:50: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    load_processed_pickle()\n",
    "except:\n",
    "    print(\"Loading train and test pickles failed. Preprocessing from scratch :(\")\n",
    "    load_csv()\n",
    "    common_words()\n",
    "    feats_train()\n",
    "    feats_test()\n",
    "    scale_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 999.0, 'nthread': 30, 'eta': 0.25, 'eval_metric': 'mlogloss', 'gamma': 0.75, 'objective': 'multi:softprob', 'subsample': 0.7000000000000001, 'max_depth': 4, 'num_class': 2, 'colsample_bytree': 0.9500000000000001, 'min_child_weight': 6.0}\n",
      "\tScore 0.3170245902848997\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 199.0, 'nthread': 30, 'eta': 0.45, 'eval_metric': 'mlogloss', 'gamma': 0.6000000000000001, 'objective': 'multi:softprob', 'subsample': 0.65, 'max_depth': 6, 'num_class': 2, 'colsample_bytree': 0.9500000000000001, 'min_child_weight': 6.0}\n",
      "\tScore 0.31412337725477474\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 747.0, 'nthread': 30, 'eta': 0.07500000000000001, 'eval_metric': 'mlogloss', 'gamma': 0.6000000000000001, 'objective': 'multi:softprob', 'subsample': 0.8, 'max_depth': 8, 'num_class': 2, 'colsample_bytree': 0.7000000000000001, 'min_child_weight': 4.0}\n",
      "\tScore 0.30365744563114555\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 119.0, 'nthread': 30, 'eta': 0.35000000000000003, 'eval_metric': 'mlogloss', 'gamma': 0.8500000000000001, 'objective': 'multi:softprob', 'subsample': 0.65, 'max_depth': 5, 'num_class': 2, 'colsample_bytree': 0.8, 'min_child_weight': 4.0}\n",
      "\tScore 0.326319731558568\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 914.0, 'nthread': 30, 'eta': 0.17500000000000002, 'eval_metric': 'mlogloss', 'gamma': 0.7000000000000001, 'objective': 'multi:softprob', 'subsample': 0.55, 'max_depth': 6, 'num_class': 2, 'colsample_bytree': 0.75, 'min_child_weight': 4.0}\n",
      "\tScore 0.3048346672735902\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 209.0, 'nthread': 30, 'eta': 0.375, 'eval_metric': 'mlogloss', 'gamma': 0.8, 'objective': 'multi:softprob', 'subsample': 0.75, 'max_depth': 11, 'num_class': 2, 'colsample_bytree': 0.65, 'min_child_weight': 3.0}\n",
      "\tScore 0.2726746525916541\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 100.0, 'nthread': 30, 'eta': 0.325, 'eval_metric': 'mlogloss', 'gamma': 0.7000000000000001, 'objective': 'multi:softprob', 'subsample': 0.5, 'max_depth': 9, 'num_class': 2, 'colsample_bytree': 0.9500000000000001, 'min_child_weight': 6.0}\n",
      "\tScore 0.30840447163517837\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 482.0, 'nthread': 30, 'eta': 0.47500000000000003, 'eval_metric': 'mlogloss', 'gamma': 0.8, 'objective': 'multi:softprob', 'subsample': 0.65, 'max_depth': 4, 'num_class': 2, 'colsample_bytree': 0.55, 'min_child_weight': 2.0}\n",
      "\tScore 0.3200386977722153\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 183.0, 'nthread': 30, 'eta': 0.07500000000000001, 'eval_metric': 'mlogloss', 'gamma': 0.5, 'objective': 'multi:softprob', 'subsample': 0.9500000000000001, 'max_depth': 10, 'num_class': 2, 'colsample_bytree': 0.9, 'min_child_weight': 4.0}\n",
      "\tScore 0.31289432199616884\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 302.0, 'nthread': 30, 'eta': 0.375, 'eval_metric': 'mlogloss', 'gamma': 0.8500000000000001, 'objective': 'multi:softprob', 'subsample': 0.8, 'max_depth': 5, 'num_class': 2, 'colsample_bytree': 0.9500000000000001, 'min_child_weight': 2.0}\n",
      "\tScore 0.31695262648294276\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 133.0, 'nthread': 30, 'eta': 0.325, 'eval_metric': 'mlogloss', 'gamma': 0.7000000000000001, 'objective': 'multi:softprob', 'subsample': 0.8, 'max_depth': 3, 'num_class': 2, 'colsample_bytree': 0.65, 'min_child_weight': 2.0}\n",
      "\tScore 0.33729301697087516\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 305.0, 'nthread': 30, 'eta': 0.07500000000000001, 'eval_metric': 'mlogloss', 'gamma': 0.7000000000000001, 'objective': 'multi:softprob', 'subsample': 0.8500000000000001, 'max_depth': 5, 'num_class': 2, 'colsample_bytree': 0.8500000000000001, 'min_child_weight': 1.0}\n",
      "\tScore 0.3308879112385065\n",
      "\n",
      "\n",
      "Training with params: \n",
      "{'silent': 0, 'n_estimators': 487.0, 'nthread': 30, 'eta': 0.1, 'eval_metric': 'mlogloss', 'gamma': 0.9, 'objective': 'multi:softprob', 'subsample': 1.0, 'max_depth': 8, 'num_class': 2, 'colsample_bytree': 0.8, 'min_child_weight': 3.0}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-62b42cb6faf3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0mtrials\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-69-62b42cb6faf3>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(trials)\u001b[0m\n\u001b[1;32m     35\u001b[0m              }\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malgo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtpe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuggest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_evals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m250\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-69-62b42cb6faf3>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_row\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_valid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/xgboost-0.6-py3.5.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m             \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBoosterUpdateOneIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "x_train, x_valid, y_train, y_valid = train_test_split(train[col], train['is_duplicate'], test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "def score(params):\n",
    "    global model, dvalid, dtrain, predictions\n",
    "    \n",
    "    print (\"Training with params: \")\n",
    "    print (params)\n",
    "    num_round = int(params['n_estimators'])\n",
    "    del params['n_estimators']\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "    # watchlist = [(dvalid, 'eval'), (dtrain, 'train')]\n",
    "    model = xgb.train(params, dtrain, num_round)\n",
    "    predictions = model.predict(dvalid).reshape((dvalid.num_row(), 2))\n",
    "    score = log_loss(y_valid, predictions)\n",
    "    print (\"\\tScore {0}\\n\\n\".format(score))\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "def optimize(trials):\n",
    "    space = {\n",
    "             'n_estimators' : hp.quniform('n_estimators', 100, 2000, 1),\n",
    "             'eta' : hp.quniform('eta', 0.025, 0.5, 0.025),\n",
    "             'max_depth' : hp.choice('max_depth', np.arange(1, 14, dtype=int)),\n",
    "             'min_child_weight' : hp.quniform('min_child_weight', 1, 6, 1),\n",
    "             'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "             'gamma' : hp.quniform('gamma', 0.5, 1, 0.05),\n",
    "             'colsample_bytree' : hp.quniform('colsample_bytree', 0.5, 1, 0.05),\n",
    "             'num_class' : 2,\n",
    "             'eval_metric': 'mlogloss',\n",
    "             'objective': \"binary:logistic\", #\"binary:logistic\"\n",
    "             'nthread' : 30,\n",
    "             'silent' : 0\n",
    "             }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=50)\n",
    "\n",
    "    print (best)\n",
    "\n",
    "#Trials object where the history of search will be stored\n",
    "trials = Trials()\n",
    "\n",
    "optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.88191992e-01,   5.11772692e-01,   5.04951186e-06, ...,\n",
       "          5.04974787e-06,   5.04942500e-06,   5.05005573e-06],\n",
       "       [  7.19469488e-01,   2.80500770e-01,   4.24165455e-06, ...,\n",
       "          4.24185282e-06,   4.24158179e-06,   4.24211157e-06],\n",
       "       [  9.99999762e-01,   2.12397268e-07,   8.22790547e-09, ...,\n",
       "          8.22828206e-09,   8.22774915e-09,   8.22878476e-09],\n",
       "       ..., \n",
       "       [  8.23094010e-01,   1.76879272e-01,   3.82033704e-06, ...,\n",
       "          3.82051212e-06,   3.82027156e-06,   3.82074541e-06],\n",
       "       [  6.76247358e-01,   3.23719233e-01,   4.77376761e-06, ...,\n",
       "          4.77399044e-06,   4.77368576e-06,   4.77428193e-06],\n",
       "       [  9.99890089e-01,   1.08783453e-04,   1.56321406e-07, ...,\n",
       "          1.56328866e-07,   1.56318720e-07,   1.56338402e-07]], dtype=float32)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(dvalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dvalid' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-51a776f97e14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdvalid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_valid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dvalid' is not defined"
     ]
    }
   ],
   "source": [
    "model.predict(dvalid).reshape((x_valid.shape[0], 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "rfr = ExtraTreesClassifier()\n",
    "\n",
    "model1 = rfr.fit(train[col], train['is_duplicate'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter optimization\n",
      "Fitting 3 folds for each of 288 candidates, totalling 864 fits\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1.5 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.452279, total= 1.7min\n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.452679, total= 1.7min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.5, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.453385, total= 1.8min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.452668, total= 1.8min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.452866, total= 1.9min\n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.453351, total= 1.9min\n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.452301, total= 1.9min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.453211, total= 2.0min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=0 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=100, min_child_weight=1.5, score=-0.452900, total= 2.0min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.445836, total= 3.6min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.446564, total= 3.6min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.445466, total= 3.6min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.445744, total= 3.6min\n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.446129, total= 3.6min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.445795, total= 3.7min\n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.446463, total= 3.7min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.445377, total= 3.7min\n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=0, score=-0.445741, total= 3.7min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=1, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1 \n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=100, min_child_weight=1, score=-0.445857, total= 3.6min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=0.75, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1, score=-0.356285, total= 6.5min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.5, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n",
      "[CV]  gamma=0, learning_rate=0.01, subsample=1, max_depth=4, colsample_bytree=0.5, n_estimators=400, min_child_weight=1, score=-0.356369, total= 6.8min\n",
      "[CV] gamma=0, learning_rate=0.01, subsample=0.75, max_depth=7, colsample_bytree=0.5, n_estimators=400, min_child_weight=1 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-5b720ea7387b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m                     \u001b[0;31m#'eta': [0.05]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                    }, verbose=3, cv=3,scoring='neg_log_loss', n_jobs=-1)\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'is_duplicate'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Best score:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \"\"\"\n\u001b[0;32m--> 945\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[1;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m--> 564\u001b[0;31m           \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    766\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/ubuntu/.local/lib/python3.5/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m                 \u001b[0;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'timeout'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    547\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "print(\"Parameter optimization\")\n",
    "xgb_model = xgb.XGBClassifier(seed=2017, objective=\"binary:logistic\")\n",
    "clf = GridSearchCV(xgb_model,\n",
    "                   {'max_depth': [4,7], # 3, 5, 8\n",
    "                    'n_estimators': [100,400], # 50,100,200\n",
    "                    'learning_rate': [0.01, 0.3],\n",
    "                    'min_child_weight': [0, 1, 1.5], # 1, 1.5, 5\n",
    "                    'gamma': [0, 0.1], # 0, 0.1\n",
    "                    'subsample': [0.5, 0.75, 1], # 0.5, 0.75, 1\n",
    "                    'colsample_bytree': [0.5, 1], # 0.5, 0.75, 1,\n",
    "                    #'eta': [0.05]\n",
    "                   }, verbose=3, cv=3,scoring='neg_log_loss', n_jobs=-1)\n",
    "clf.fit(train[col], train['is_duplicate'])\n",
    "print(str(int(time.time() - t0)))\n",
    "print(\"Best score:\")\n",
    "print(clf.best_score_)\n",
    "print(\"Best parameters:\")\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\ttrain-logloss:0.536157\tvalid-logloss:0.5367\n",
      "Multiple eval metrics have been passed: 'valid-logloss' will be used for early stopping.\n",
      "\n",
      "Will train until valid-logloss hasn't improved in 50 rounds.\n",
      "[100]\ttrain-logloss:0.321855\tvalid-logloss:0.3295\n",
      "[200]\ttrain-logloss:0.311755\tvalid-logloss:0.323944\n",
      "[300]\ttrain-logloss:0.304759\tvalid-logloss:0.321192\n",
      "[400]\ttrain-logloss:0.298762\tvalid-logloss:0.318791\n",
      "[500]\ttrain-logloss:0.293159\tvalid-logloss:0.316835\n",
      "[600]\ttrain-logloss:0.288136\tvalid-logloss:0.315121\n",
      "[700]\ttrain-logloss:0.283651\tvalid-logloss:0.313787\n",
      "[800]\ttrain-logloss:0.279235\tvalid-logloss:0.312433\n",
      "[900]\ttrain-logloss:0.275045\tvalid-logloss:0.311331\n",
      "[1000]\ttrain-logloss:0.270962\tvalid-logloss:0.309929\n",
      "[1100]\ttrain-logloss:0.267324\tvalid-logloss:0.30887\n",
      "[1200]\ttrain-logloss:0.263649\tvalid-logloss:0.307913\n",
      "[1300]\ttrain-logloss:0.260274\tvalid-logloss:0.306956\n",
      "[1400]\ttrain-logloss:0.256753\tvalid-logloss:0.305865\n",
      "[1500]\ttrain-logloss:0.253261\tvalid-logloss:0.304648\n",
      "[1600]\ttrain-logloss:0.250083\tvalid-logloss:0.304158\n",
      "[1700]\ttrain-logloss:0.247008\tvalid-logloss:0.303241\n",
      "[1800]\ttrain-logloss:0.24408\tvalid-logloss:0.302505\n",
      "[1900]\ttrain-logloss:0.241301\tvalid-logloss:0.3019\n",
      "[2000]\ttrain-logloss:0.238136\tvalid-logloss:0.300973\n",
      "[2100]\ttrain-logloss:0.235429\tvalid-logloss:0.30012\n",
      "[2200]\ttrain-logloss:0.232846\tvalid-logloss:0.299511\n",
      "[2300]\ttrain-logloss:0.230122\tvalid-logloss:0.298653\n",
      "[2400]\ttrain-logloss:0.227438\tvalid-logloss:0.297883\n",
      "[2500]\ttrain-logloss:0.225011\tvalid-logloss:0.297342\n",
      "[2600]\ttrain-logloss:0.222692\tvalid-logloss:0.296647\n",
      "[2700]\ttrain-logloss:0.220246\tvalid-logloss:0.295836\n",
      "[2800]\ttrain-logloss:0.218021\tvalid-logloss:0.295492\n",
      "[2900]\ttrain-logloss:0.215762\tvalid-logloss:0.295064\n",
      "[3000]\ttrain-logloss:0.213465\tvalid-logloss:0.294489\n",
      "[3100]\ttrain-logloss:0.211298\tvalid-logloss:0.294088\n",
      "[3200]\ttrain-logloss:0.209006\tvalid-logloss:0.293577\n",
      "[3300]\ttrain-logloss:0.206742\tvalid-logloss:0.292978\n",
      "[3400]\ttrain-logloss:0.20458\tvalid-logloss:0.29245\n",
      "[3500]\ttrain-logloss:0.202477\tvalid-logloss:0.291807\n",
      "[3600]\ttrain-logloss:0.200399\tvalid-logloss:0.291387\n",
      "[3700]\ttrain-logloss:0.198361\tvalid-logloss:0.290729\n",
      "[3800]\ttrain-logloss:0.196408\tvalid-logloss:0.290217\n",
      "[3900]\ttrain-logloss:0.194422\tvalid-logloss:0.289739\n",
      "[4000]\ttrain-logloss:0.192472\tvalid-logloss:0.289391\n",
      "[4100]\ttrain-logloss:0.190559\tvalid-logloss:0.288739\n",
      "[4200]\ttrain-logloss:0.188851\tvalid-logloss:0.28846\n",
      "[4300]\ttrain-logloss:0.187111\tvalid-logloss:0.288013\n",
      "[4400]\ttrain-logloss:0.185321\tvalid-logloss:0.287591\n",
      "[4500]\ttrain-logloss:0.183458\tvalid-logloss:0.28731\n",
      "[4600]\ttrain-logloss:0.181667\tvalid-logloss:0.286746\n",
      "[4700]\ttrain-logloss:0.179892\tvalid-logloss:0.286477\n",
      "[4800]\ttrain-logloss:0.178155\tvalid-logloss:0.285943\n",
      "[4900]\ttrain-logloss:0.176492\tvalid-logloss:0.285613\n",
      "[4999]\ttrain-logloss:0.174841\tvalid-logloss:0.28511\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params['eval_metric'] = 'logloss'\n",
    "params[\"eta\"] = 0.375 # 0.05\n",
    "params[\"subsample\"] = 0.75 # 1.0\n",
    "params[\"min_child_weight\"] = 3\n",
    "params[\"colsample_bytree\"] = 0.65 # 1.0\n",
    "params[\"max_depth\"] = 5 # 5\n",
    "params[\"silent\"] = 1\n",
    "params[\"seed\"] = 2017\n",
    "\n",
    "\n",
    "\n",
    "d_train = xgb.DMatrix(x_train, label=y_train)\n",
    "d_valid = xgb.DMatrix(x_valid, label=y_valid)\n",
    "watchlist = [(d_train, 'train'), (d_valid, 'valid')]\n",
    "bst = xgb.train(params, d_train, 5000, watchlist, early_stopping_rounds=50, verbose_eval=100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.196895026841\n"
     ]
    }
   ],
   "source": [
    "d_test = xgb.DMatrix(test[col])\n",
    "p_test = bst.predict(d_test)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = test['test_id']\n",
    "sub['is_duplicate'] = p_test\n",
    "\n",
    "#df['is_duplicate'] = df['is_duplicate'].map(lambda x: 0.000000000001 if x < 0.0001 else x)\n",
    "#df['is_duplicate'] = df['is_duplicate'].map(lambda x: 0.999999999999 if x > 0.98 else x)\n",
    "\n",
    "sub.to_csv('./submissions/submission_xgb_0424_aeron2.csv', index=False)\n",
    "print(log_loss(train.is_duplicate, bst.predict(xgb.DMatrix(train[col]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_id</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>test</th>\n",
       "      <th>test1</th>\n",
       "      <th>question1_nouns</th>\n",
       "      <th>question2_nouns</th>\n",
       "      <th>question1_proper_nouns</th>\n",
       "      <th>question2_proper_nouns</th>\n",
       "      <th>question1_verbs</th>\n",
       "      <th>...</th>\n",
       "      <th>question1_cd</th>\n",
       "      <th>question2_cd</th>\n",
       "      <th>question1_dt</th>\n",
       "      <th>question2_dt</th>\n",
       "      <th>question1_ex</th>\n",
       "      <th>question2_ex</th>\n",
       "      <th>question1_ls</th>\n",
       "      <th>question2_ls</th>\n",
       "      <th>question1_wdt</th>\n",
       "      <th>question2_wdt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How does the Surface Pro himself 4 compare wit...</td>\n",
       "      <td>Why did Microsoft choose core m3 and not core ...</td>\n",
       "      <td>[(how, WRB), (does, VBZ), (the, DT), (surface,...</td>\n",
       "      <td>[(why, WRB), (did, VBD), (microsoft, VB), (cho...</td>\n",
       "      <td>[surface, compare]</td>\n",
       "      <td>[core, m3, i3, home, surface]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[does, pro]</td>\n",
       "      <td>...</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[4]</td>\n",
       "      <td>[the]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Should I have a hair transplant at age 24? How...</td>\n",
       "      <td>How much cost does hair transplant require?</td>\n",
       "      <td>[(should, MD), (i, VB), (have, VBP), (a, DT), ...</td>\n",
       "      <td>[(how, WRB), (much, JJ), (cost, NN), (does, VB...</td>\n",
       "      <td>[hair, transplant, age]</td>\n",
       "      <td>[cost, require]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[i, have, cost]</td>\n",
       "      <td>...</td>\n",
       "      <td>[24]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[a]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>What but is the best way to send money from Ch...</td>\n",
       "      <td>What you send money to China?</td>\n",
       "      <td>[(what, WP), (but, CC), (is, VBZ), (the, DT), ...</td>\n",
       "      <td>[(what, WP), (you, PRP), (send, VBP), (money, ...</td>\n",
       "      <td>[way, money, china]</td>\n",
       "      <td>[money]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[is, send]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[the, the]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Which food not emulsifiers?</td>\n",
       "      <td>What foods fibre?</td>\n",
       "      <td>[(which, WDT), (food, NN), (not, RB), (emulsif...</td>\n",
       "      <td>[(what, WP), (foods, VBZ), (fibre, NN), (?, .)]</td>\n",
       "      <td>[food]</td>\n",
       "      <td>[fibre]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[which]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How \"aberystwyth\" start reading?</td>\n",
       "      <td>How their can I start reading?</td>\n",
       "      <td>[(how, WRB), (``, ``), (aberystwyth, NN), ('',...</td>\n",
       "      <td>[(how, WRB), (their, PRP$), (can, MD), (i, VB)...</td>\n",
       "      <td>[aberystwyth, start, reading]</td>\n",
       "      <td>[reading]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   test_id                                          question1  \\\n",
       "0        0  How does the Surface Pro himself 4 compare wit...   \n",
       "1        1  Should I have a hair transplant at age 24? How...   \n",
       "2        2  What but is the best way to send money from Ch...   \n",
       "3        3                        Which food not emulsifiers?   \n",
       "4        4                   How \"aberystwyth\" start reading?   \n",
       "\n",
       "                                           question2  \\\n",
       "0  Why did Microsoft choose core m3 and not core ...   \n",
       "1        How much cost does hair transplant require?   \n",
       "2                      What you send money to China?   \n",
       "3                                  What foods fibre?   \n",
       "4                     How their can I start reading?   \n",
       "\n",
       "                                                test  \\\n",
       "0  [(how, WRB), (does, VBZ), (the, DT), (surface,...   \n",
       "1  [(should, MD), (i, VB), (have, VBP), (a, DT), ...   \n",
       "2  [(what, WP), (but, CC), (is, VBZ), (the, DT), ...   \n",
       "3  [(which, WDT), (food, NN), (not, RB), (emulsif...   \n",
       "4  [(how, WRB), (``, ``), (aberystwyth, NN), ('',...   \n",
       "\n",
       "                                               test1  \\\n",
       "0  [(why, WRB), (did, VBD), (microsoft, VB), (cho...   \n",
       "1  [(how, WRB), (much, JJ), (cost, NN), (does, VB...   \n",
       "2  [(what, WP), (you, PRP), (send, VBP), (money, ...   \n",
       "3    [(what, WP), (foods, VBZ), (fibre, NN), (?, .)]   \n",
       "4  [(how, WRB), (their, PRP$), (can, MD), (i, VB)...   \n",
       "\n",
       "                 question1_nouns                question2_nouns  \\\n",
       "0             [surface, compare]  [core, m3, i3, home, surface]   \n",
       "1        [hair, transplant, age]                [cost, require]   \n",
       "2            [way, money, china]                        [money]   \n",
       "3                         [food]                        [fibre]   \n",
       "4  [aberystwyth, start, reading]                      [reading]   \n",
       "\n",
       "  question1_proper_nouns question2_proper_nouns  question1_verbs  \\\n",
       "0                     []                     []      [does, pro]   \n",
       "1                     []                     []  [i, have, cost]   \n",
       "2                     []                     []       [is, send]   \n",
       "3                     []                     []               []   \n",
       "4                     []                     []               []   \n",
       "\n",
       "       ...      question1_cd question2_cd question1_dt question2_dt  \\\n",
       "0      ...               [4]          [4]        [the]           []   \n",
       "1      ...              [24]           []          [a]           []   \n",
       "2      ...                []           []   [the, the]           []   \n",
       "3      ...                []           []           []           []   \n",
       "4      ...                []           []           []           []   \n",
       "\n",
       "  question1_ex question2_ex question1_ls question2_ls question1_wdt  \\\n",
       "0           []           []           []           []            []   \n",
       "1           []           []           []           []            []   \n",
       "2           []           []           []           []            []   \n",
       "3           []           []           []           []       [which]   \n",
       "4           []           []           []           []            []   \n",
       "\n",
       "  question2_wdt  \n",
       "0            []  \n",
       "1            []  \n",
       "2            []  \n",
       "3            []  \n",
       "4            []  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
